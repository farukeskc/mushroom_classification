{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can go wrong if we have a wide range of numbers in our input/output data and we don't do any pre-processing on them and feed the neural network with unprocessed data\n",
      "\n",
      "If one of the variables is in the range of 1000’s and another is in the range of 0.1’s then the coefficient found by neural network for the first variable would be (most likely) much larger than the the other. This does not really show if the first variable is more important or not, but this coefficient needs to be large just to compensate for the scale of that variable\n"
     ]
    }
   ],
   "source": [
    "print(\"What can go wrong if we have a wide range of numbers in our input/output data and we don't do any pre-processing on them and feed the neural network with unprocessed data\\n\")\n",
    "\n",
    "print(\"If one of the variables is in the range of 1000’s and another is in the range of 0.1’s then the coefficient found by neural network for the first variable would be (most likely) much larger than the the other. This does not really show if the first variable is more important or not, but this coefficient needs to be large just to compensate for the scale of that variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do we tackle this problem? (Hint: Normalization, standardization)\n",
      "\n",
      "To handle this issue we rescale the input variable in range (0,1) by using normalization or we can standardize the values to have a mean of 0 and a standard deviation of 1\n"
     ]
    }
   ],
   "source": [
    "print(\"How do we tackle this problem? (Hint: Normalization, standardization)\\n\")\n",
    "\n",
    "print(\"To handle this issue we rescale the input variable in range (0,1) by using normalization or we can standardize the values to have a mean of 0 and a standard deviation of 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"mushrooms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    \n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from layer import Layer\n",
    "import numpy as np\n",
    "\n",
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of edges that connects to neurons in next layer\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from layer import Layer\n",
    "\n",
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a function for calculating softmax for a list of numbers\n",
    "from numpy import exp\n",
    " \n",
    "# calculate the softmax of a vector\n",
    "def softmax(vector):\n",
    "    e = exp(vector)\n",
    "    return e / e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "        \n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network \n",
    "    \n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        '''\n",
    "        Fit function does the training. \n",
    "        Training data is passed 1-by-1 through the network layers during forward propagation.\n",
    "        Loss (error) is calculated for each input and back propagation is performed via partial \n",
    "        derivatives on each layer.\n",
    "        '''\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve MNIST wihout normalization and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/35   error=0.282392\n",
      "epoch 2/35   error=0.132190\n",
      "epoch 3/35   error=0.124999\n",
      "epoch 4/35   error=0.120291\n",
      "epoch 5/35   error=0.116706\n",
      "epoch 6/35   error=0.114691\n",
      "epoch 7/35   error=0.115094\n",
      "epoch 8/35   error=0.115340\n",
      "epoch 9/35   error=0.113926\n",
      "epoch 10/35   error=0.112681\n",
      "epoch 11/35   error=0.111852\n",
      "epoch 12/35   error=0.111107\n",
      "epoch 13/35   error=0.110214\n",
      "epoch 14/35   error=0.109581\n",
      "epoch 15/35   error=0.109005\n",
      "epoch 16/35   error=0.108457\n",
      "epoch 17/35   error=0.108993\n",
      "epoch 18/35   error=0.110814\n",
      "epoch 19/35   error=0.110420\n",
      "epoch 20/35   error=0.110062\n",
      "epoch 21/35   error=0.109862\n",
      "epoch 22/35   error=0.109317\n",
      "epoch 23/35   error=0.108888\n",
      "epoch 24/35   error=0.108505\n",
      "epoch 25/35   error=0.108150\n",
      "epoch 26/35   error=0.107813\n",
      "epoch 27/35   error=0.107490\n",
      "epoch 28/35   error=0.107178\n",
      "epoch 29/35   error=0.106877\n",
      "epoch 30/35   error=0.106583\n",
      "epoch 31/35   error=0.106296\n",
      "epoch 32/35   error=0.106015\n",
      "epoch 33/35   error=0.105739\n",
      "epoch 34/35   error=0.105453\n",
      "epoch 35/35   error=0.105169\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#from network import Network\n",
    "#from fc_layer import FCLayer\n",
    "#from activation_layer import ActivationLayer\n",
    "#from activations import tanh, tanh_prime\n",
    "#from losses import mse, mse_prime\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "#x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "#x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "y_pred = net.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_argmax = []\n",
    "for i in range(len(y_pred)):\n",
    "    y_pred_argmax.append(np.argmax(y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_argmax = []\n",
    "for i in range(len(y_test)):\n",
    "    y_test_argmax.append(np.argmax(y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2148"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_pred_argmax,y_test_argmax)\n",
    "report['w/o_any'] = accuracy\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve MNIST with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    for sample in data:\n",
    "        sample_max = sample[0].max()\n",
    "        sample[0] = sample[0]/sample_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 784)\n",
      "epoch 1/35   error=0.235308\n",
      "epoch 2/35   error=0.094617\n",
      "epoch 3/35   error=0.072925\n",
      "epoch 4/35   error=0.060449\n",
      "epoch 5/35   error=0.051586\n",
      "epoch 6/35   error=0.044783\n",
      "epoch 7/35   error=0.039248\n",
      "epoch 8/35   error=0.034798\n",
      "epoch 9/35   error=0.031083\n",
      "epoch 10/35   error=0.028240\n",
      "epoch 11/35   error=0.026067\n",
      "epoch 12/35   error=0.024224\n",
      "epoch 13/35   error=0.022646\n",
      "epoch 14/35   error=0.021264\n",
      "epoch 15/35   error=0.020058\n",
      "epoch 16/35   error=0.018976\n",
      "epoch 17/35   error=0.017990\n",
      "epoch 18/35   error=0.017132\n",
      "epoch 19/35   error=0.016290\n",
      "epoch 20/35   error=0.015520\n",
      "epoch 21/35   error=0.014716\n",
      "epoch 22/35   error=0.013975\n",
      "epoch 23/35   error=0.013295\n",
      "epoch 24/35   error=0.012632\n",
      "epoch 25/35   error=0.012001\n",
      "epoch 26/35   error=0.011380\n",
      "epoch 27/35   error=0.010866\n",
      "epoch 28/35   error=0.010388\n",
      "epoch 29/35   error=0.010005\n",
      "epoch 30/35   error=0.009624\n",
      "epoch 31/35   error=0.009311\n",
      "epoch 32/35   error=0.008977\n",
      "epoch 33/35   error=0.008712\n",
      "epoch 34/35   error=0.008446\n",
      "epoch 35/35   error=0.008250\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#from network import Network\n",
    "#from fc_layer import FCLayer\n",
    "#from activation_layer import ActivationLayer\n",
    "#from activations import tanh, tanh_prime\n",
    "#from losses import mse, mse_prime\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "print(x_train.shape)\n",
    "normalize(x_train)\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "normalize(x_test)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "y_pred = net.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_argmax = []\n",
    "for i in range(len(y_pred)):\n",
    "    y_pred_argmax.append(np.argmax(y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_argmax = []\n",
    "for i in range(len(y_test)):\n",
    "    y_test_argmax.append(np.argmax(y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.794"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_pred_argmax,y_test_argmax)\n",
    "report['normalization'] = accuracy\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve MNIST with standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    for sample in data:\n",
    "        sample_mean = sample[0].mean()\n",
    "        sample_std = sample[0].std()\n",
    "        sample[0] = (sample[0]-sample_mean)/sample_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/35   error=0.277231\n",
      "epoch 2/35   error=0.098760\n",
      "epoch 3/35   error=0.075744\n",
      "epoch 4/35   error=0.063000\n",
      "epoch 5/35   error=0.053754\n",
      "epoch 6/35   error=0.046423\n",
      "epoch 7/35   error=0.041075\n",
      "epoch 8/35   error=0.036868\n",
      "epoch 9/35   error=0.033681\n",
      "epoch 10/35   error=0.031078\n",
      "epoch 11/35   error=0.028855\n",
      "epoch 12/35   error=0.026857\n",
      "epoch 13/35   error=0.025311\n",
      "epoch 14/35   error=0.023879\n",
      "epoch 15/35   error=0.022621\n",
      "epoch 16/35   error=0.021457\n",
      "epoch 17/35   error=0.020282\n",
      "epoch 18/35   error=0.019103\n",
      "epoch 19/35   error=0.018216\n",
      "epoch 20/35   error=0.017382\n",
      "epoch 21/35   error=0.016731\n",
      "epoch 22/35   error=0.016009\n",
      "epoch 23/35   error=0.015379\n",
      "epoch 24/35   error=0.014705\n",
      "epoch 25/35   error=0.014239\n",
      "epoch 26/35   error=0.013771\n",
      "epoch 27/35   error=0.013331\n",
      "epoch 28/35   error=0.012926\n",
      "epoch 29/35   error=0.012480\n",
      "epoch 30/35   error=0.012342\n",
      "epoch 31/35   error=0.011868\n",
      "epoch 32/35   error=0.011377\n",
      "epoch 33/35   error=0.010971\n",
      "epoch 34/35   error=0.010543\n",
      "epoch 35/35   error=0.010355\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#from network import Network\n",
    "#from fc_layer import FCLayer\n",
    "#from activation_layer import ActivationLayer\n",
    "#from activations import tanh, tanh_prime\n",
    "#from losses import mse, mse_prime\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "standardize(x_train)\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "normalize(x_test)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "y_pred = net.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_argmax = []\n",
    "for i in range(len(y_pred)):\n",
    "    y_pred_argmax.append(np.argmax(y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_argmax = []\n",
    "for i in range(len(y_test)):\n",
    "    y_test_argmax.append(np.argmax(y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6249"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_pred_argmax,y_test_argmax)\n",
    "report['standardization'] = accuracy\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w/o_any': 0.2148, 'normalization': 0.794, 'standardization': 0.6249}\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "074a5fe0053dd021236f50215ade0b52b6b19c0039fb0d4373b25a5fb86fc58b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
